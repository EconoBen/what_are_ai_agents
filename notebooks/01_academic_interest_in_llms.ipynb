{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Academic Interest in LLMs\n",
    "\n",
    "**Purpose**: <br>\n",
    "<br>\n",
    "To use [arxiv](https://arxiv.org/) metadata to track the interest in LLMs by way word-count references.\n",
    "\n",
    "**Instructions**: <br>\n",
    "1. To follow along yourself, you will need access to a [Kaggle](https://www.kaggle.com/) account. after signing up, download your API key from account -> settings -> \"Create New Token\". Ensure the downloaded `kaggle.json` file, which contains your username and API key, is placed in a `.kaggle` folder in your root directory. e.g. `mv ~/Downloads/ ~/.kaggle/kaggle.json`.\n",
    "\n",
    "2. Ensure your kaggle file has the correct permissions: `chmod 600 ~/.kaggle/kaggle.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arxiv.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
     ]
    }
   ],
   "source": [
    "! kaggle datasets download -p ../data/ cornell-university/arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ../data/arxiv.zip\n",
      "  inflating: ../data/arxiv-metadata-oai-snapshot.json  \n"
     ]
    }
   ],
   "source": [
    "! unzip ../data/arxiv.zip -d ../data/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we convert the just into a list of dictionaries instead of a string representation of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import loads\n",
    "\n",
    "with open('../data/arxiv-metadata-oai-snapshot.json', 'r') as f:\n",
    "    data = [loads(line) for line in f]\n",
    "    f.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if anything from title or abstract, contains information about LLM or closely-related subjects such as transformers..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7391"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Dict\n",
    "def check_for_references(meta_data: Dict[str, str]):\n",
    "    \"\"\"If none of the key phrases are in the text, return False\"\"\"\n",
    "    key_phrases = [\n",
    "                    \"large language model\",\n",
    "                    \"large language models\",\n",
    "                    \"LLM\",\n",
    "                    \"LLMs\",\n",
    "                    \"Attention Is All You Need\",\n",
    "                    \"generative ai\",\n",
    "                    \"GPT-3\",\n",
    "                    \"GPT-4\",\n",
    "                    \"OpenAI\",\n",
    "                    \"Transformer architecture\",\n",
    "                    \"transformers\",\n",
    "                    \"self-attention\",\n",
    "                ]\n",
    "    bert_like_models = set([\n",
    "                        \"BERT\",\n",
    "                        \"RoBERTa\",\n",
    "                        \"DistilBERT\",\n",
    "                        \"ALBERT\",\n",
    "                        \"SpanBERT\",\n",
    "                        \"BioBERT\",\n",
    "                        \"SciBERT\",\n",
    "                        \"CamemBERT\",\n",
    "                        \"TurkuBERT\",\n",
    "                        \"MobileBERT\",\n",
    "                        \"TinyBERT\",\n",
    "                        \"ELECTRA\",\n",
    "                        \"DeBERTa\"\n",
    "                    ])\n",
    "\n",
    "    key_phrases = set([phrase.lower() for phrase in key_phrases])\n",
    "\n",
    "    title: str = meta_data['title'].replace('\\n', ' ').lower().split()\n",
    "    abstract: str = meta_data['abstract'].replace('\\n', ' ').lower().split()\n",
    "\n",
    "    for phrase in key_phrases:\n",
    "        if (phrase in title) or (phrase in abstract):\n",
    "            return True\n",
    "        \n",
    "    for phrase in  bert_like_models:\n",
    "        if (phrase in title) or (phrase in abstract):\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "sum([1 for doc in data if check_for_references(doc)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so we know there are plenty of scholarly interest in the above. Frankly, it'd be surprising if there weren't... \n",
    "\n",
    "Next, let's try to find a timeline of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>submitter</th>\n",
       "      <th>authors</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>journal-ref</th>\n",
       "      <th>doi</th>\n",
       "      <th>report-no</th>\n",
       "      <th>categories</th>\n",
       "      <th>license</th>\n",
       "      <th>abstract</th>\n",
       "      <th>versions</th>\n",
       "      <th>update_date</th>\n",
       "      <th>authors_parsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0704.0001</td>\n",
       "      <td>Pavel Nadolsky</td>\n",
       "      <td>C. Bal\\'azs, E. L. Berger, P. M. Nadolsky, C.-...</td>\n",
       "      <td>Calculation of prompt diphoton production cros...</td>\n",
       "      <td>37 pages, 15 figures; published version</td>\n",
       "      <td>Phys.Rev.D76:013009,2007</td>\n",
       "      <td>10.1103/PhysRevD.76.013009</td>\n",
       "      <td>ANL-HEP-PR-07-12</td>\n",
       "      <td>hep-ph</td>\n",
       "      <td>None</td>\n",
       "      <td>A fully differential calculation in perturba...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Mon, 2 Apr 2007...</td>\n",
       "      <td>2008-11-26</td>\n",
       "      <td>[[Balázs, C., ], [Berger, E. L., ], [Nadolsky,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0704.0002</td>\n",
       "      <td>Louis Theran</td>\n",
       "      <td>Ileana Streinu and Louis Theran</td>\n",
       "      <td>Sparsity-certifying Graph Decompositions</td>\n",
       "      <td>To appear in Graphs and Combinatorics</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>math.CO cs.CG</td>\n",
       "      <td>http://arxiv.org/licenses/nonexclusive-distrib...</td>\n",
       "      <td>We describe a new algorithm, the $(k,\\ell)$-...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Sat, 31 Mar 200...</td>\n",
       "      <td>2008-12-13</td>\n",
       "      <td>[[Streinu, Ileana, ], [Theran, Louis, ]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0704.0003</td>\n",
       "      <td>Hongjun Pan</td>\n",
       "      <td>Hongjun Pan</td>\n",
       "      <td>The evolution of the Earth-Moon system based o...</td>\n",
       "      <td>23 pages, 3 figures</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>physics.gen-ph</td>\n",
       "      <td>None</td>\n",
       "      <td>The evolution of Earth-Moon system is descri...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Sun, 1 Apr 2007...</td>\n",
       "      <td>2008-01-13</td>\n",
       "      <td>[[Pan, Hongjun, ]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0704.0004</td>\n",
       "      <td>David Callan</td>\n",
       "      <td>David Callan</td>\n",
       "      <td>A determinant of Stirling cycle numbers counts...</td>\n",
       "      <td>11 pages</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>math.CO</td>\n",
       "      <td>None</td>\n",
       "      <td>We show that a determinant of Stirling cycle...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Sat, 31 Mar 200...</td>\n",
       "      <td>2007-05-23</td>\n",
       "      <td>[[Callan, David, ]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0704.0005</td>\n",
       "      <td>Alberto Torchinsky</td>\n",
       "      <td>Wael Abu-Shammala and Alberto Torchinsky</td>\n",
       "      <td>From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...</td>\n",
       "      <td>None</td>\n",
       "      <td>Illinois J. Math. 52 (2008) no.2, 681-689</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>math.CA math.FA</td>\n",
       "      <td>None</td>\n",
       "      <td>In this paper we show how to compute the $\\L...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Mon, 2 Apr 2007...</td>\n",
       "      <td>2013-10-15</td>\n",
       "      <td>[[Abu-Shammala, Wael, ], [Torchinsky, Alberto, ]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  ...                                     authors_parsed\n",
       "0  0704.0001  ...  [[Balázs, C., ], [Berger, E. L., ], [Nadolsky,...\n",
       "1  0704.0002  ...           [[Streinu, Ileana, ], [Theran, Louis, ]]\n",
       "2  0704.0003  ...                                 [[Pan, Hongjun, ]]\n",
       "3  0704.0004  ...                                [[Callan, David, ]]\n",
       "4  0704.0005  ...  [[Abu-Shammala, Wael, ], [Torchinsky, Alberto, ]]\n",
       "\n",
       "[5 rows x 14 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "# First, we load the data into a dataframe. Since it's a list of dictionaries, we can use the from_records function.\n",
    "\n",
    "metadata = DataFrame.from_records(data)\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, I'd like to determine when the _first_ version of these articles were published, so we'll parse out that information\n",
    "\n",
    "v1_dates = [version[0]['created'] for version in metadata['versions'] if version[0]['version'] == 'v1']\n",
    "metadata['v1_dates'] = v1_dates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: What's nice about the above is, despite putting a condition of version == 'v1', since the dataframe was able to create a new column, we know implcitly there were no articles missing a v1 date. Python would have thrown an error saying v1_dates was too short, otherwise. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's add a boolean to each row that contains an LLM-like reference. We will use these later..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata['is_LLM'] = [1 if check_for_references(doc) else 0 for doc in data]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if our results make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3918</th>\n",
       "      <td>On over-reflection and generation of Gravito-A...</td>\n",
       "      <td>The dynamics of linear perturbations is stud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6210</th>\n",
       "      <td>IIB backgrounds with five-form flux</td>\n",
       "      <td>We investigate all N=2 supersymmetric IIB su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8902</th>\n",
       "      <td>Anatomy of bubbling solutions</td>\n",
       "      <td>We present a comprehensive analysis of holog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12701</th>\n",
       "      <td>Self-Stabilizing Wavelets and r-Hops Coordination</td>\n",
       "      <td>We introduce a simple tool called the wavele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17627</th>\n",
       "      <td>Long-time stable HTSC DC-SQUID gradiometers wi...</td>\n",
       "      <td>In applications for high-Tc superconducting ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title                                           abstract\n",
       "3918   On over-reflection and generation of Gravito-A...    The dynamics of linear perturbations is stud...\n",
       "6210                 IIB backgrounds with five-form flux    We investigate all N=2 supersymmetric IIB su...\n",
       "8902                       Anatomy of bubbling solutions    We present a comprehensive analysis of holog...\n",
       "12701  Self-Stabilizing Wavelets and r-Hops Coordination    We introduce a simple tool called the wavele...\n",
       "17627  Long-time stable HTSC DC-SQUID gradiometers wi...    In applications for high-Tc superconducting ..."
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata[metadata['is_LLM'] == 1][['title', 'abstract']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  We investigate all N=2 supersymmetric IIB supergravity backgrounds with\\nnon-vanishing five-form flux. The Killing spinors have stability subgroups\\n$Spin(7)\\\\ltimes\\\\bR^8$, $SU(4)\\\\ltimes\\\\bR^8$ and $G_2$. In the\\n$SU(4)\\\\ltimes\\\\bR^8$ case, two different types of geometry arise depending on\\nwhether the Killing spinors are generic or pure. In both cases, the backgrounds\\nadmit a null Killing vector field which leaves invariant the $SU(4)\\\\ltimes\\n\\\\bR^8$ structure, and an almost complex structure in the directions transverse\\nto the lightcone. In the generic case, the twist of the vector field is trivial\\nbut the almost complex structure is non-integrable, while in the pure case the\\ntwist is non-trivial but the almost complex structure is integrable and\\nassociated with a relatively balanced Hermitian structure. The $G_2$\\nbackgrounds admit a time-like Killing vector field and two spacelike closed\\none-forms, and the seven directions transverse to these admit a co-symplectic\\n$G_2$ structure. The $Spin(7)\\\\ltimes\\\\bR^8$ backgrounds are pp-waves propagating\\nin an eight-dimensional manifold with holonomy $Spin(7)$. In addition we show\\nthat all the supersymmetric solutions of simple five-dimensional supergravity\\nwith a time-like Killing vector field, which include the $AdS_5$ black holes,\\nlift to $SU(4)\\\\ltimes\\\\bR^8$ pure Killing spinor IIB backgrounds. We also show\\nthat the LLM solution is associated with a co-symplectic co-homogeneity one\\n$G_2$ manifold which has principal orbit $S^3\\\\times S^3$.\\n'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata[metadata['is_LLM'] == 1][['title', 'abstract']].iat[1, 1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already we see that the results are full of false positives. Let's use something a bit more adanced, and relative to our work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Conversation, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model = \"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "model = BlenderbotSmallForConditionalGeneration.from_pretrained('facebook/blenderbot_small-90M')\n",
    "\n",
    "# Let's say we have a list of abstracts\n",
    "abstracts = [\n",
    "    'This paper presents a new approach to generative AI using large language models.',\n",
    "    'We propose a novel architecture for transformer models in NLP tasks.',\n",
    "    # Add more abstracts here...\n",
    "]\n",
    "\n",
    "# Create a system persona that explains the task\n",
    "system_persona = \"Your task is to classify whether an abstract is about Large Language Models (LLMs) and Generative AI, or not.\"\n",
    "\n",
    "\n",
    "for abstract in abstracts:\n",
    "    # Create a conversation with the system persona and the abstract\n",
    "    conversation = Conversation(system_persona + \"\\n\" + abstract)\n",
    "\n",
    "    # Generate a response from the model\n",
    "    model_input = tokenizer(conversation, return_tensors='pt')\n",
    "    model_output = model.generate(**model_input)\n",
    "    response = tokenizer.decode(model_output[:, model_input['input_ids'].shape[-1]:][0], skip_special_tokens=True)\n",
    "\n",
    "    # Print the response\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('tiiuae/falcon-7b-instruct')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('tiiuae/falcon-7b-instruct')\n",
    "\n",
    "# Generate text\n",
    "def generate_text(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    output = model.generate(input_ids, max_length=1024, do_sample=True)\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "input_text = \"Hello, how are you?\"\n",
    "output_text = generate_text(input_text)\n",
    "print(output_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, now let's start grouping by monthly counts using my favorite pandas function, pd.Grouper. For more, you can check my article [here](https://benjaminlabaschin.com/pandas-functions-advanced-groupbys-with-grouper-assign-and-query/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         2007-04-02 19:18:42\n",
       "1         2007-03-31 02:26:18\n",
       "2         2007-04-01 20:46:54\n",
       "3         2007-03-31 03:16:14\n",
       "4         2007-04-02 18:09:58\n",
       "                  ...        \n",
       "2263487   1996-08-26 15:08:35\n",
       "2263488   1996-08-31 17:34:38\n",
       "2263489   1996-09-03 14:08:26\n",
       "2263490   1996-09-18 07:57:29\n",
       "2263491   1996-09-25 14:17:09\n",
       "Name: v1_dates, Length: 2263492, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import Grouper, to_datetime\n",
    "\n",
    "# first let's convert to a datetime object\n",
    "metadata['v1_dates'] = to_datetime(metadata['v1_dates'])\n",
    "\n",
    "metadata.groupby(Grouper(key='v1_dates', freq='1m'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
